library(dplyr)         # for basic data wrangling
# Modeling packages
library(keras)         # for fitting DNNs
library(tfruns)        # for additional grid search & model training functions
# Modeling helper package - not necessary for reproducibility
library(tfestimators) 
library(tensorflow)


# reading of Dataset 
mnist <- dataset_mnist()
x_train <- mnist$train$x
y_train <- mnist$train$y
x_test <- mnist$test$x
y_test <- mnist$test$y

x_train <- array_reshape(x_train, c(nrow(x_train), 784))
x_test <- array_reshape(x_test, c(nrow(x_test), 784))
# rescale
x_train <- x_train / 255
x_test <- x_test / 255

y_train <- to_categorical(y_train, 10)
y_test <- to_categorical(y_test, 10)



# Declaring Flags
FLAGS <- flags( flag_numeric("nodes1", 256),
                flag_numeric("nodes2", 128),
                flag_numeric("nodes3", 64),
                # Dropout
                flag_numeric("dropout1", 0.4),
                flag_numeric("dropout2", 0.3),
                flag_numeric("dropout3", 0.2),
                # Learning paramaters
                flag_string("optimizer", "rmsprop"),
                flag_numeric("lr_annealing", 0.1))


# Model using neural network 
model <- keras_model_sequential() %>%
  layer_dense(units = FLAGS$nodes1, activation = "relu", input_shape = ncol(x_train)) %>%
  layer_batch_normalization() %>%
  layer_dropout(rate = FLAGS$dropout1) %>%
  layer_dense(units = FLAGS$nodes2, activation = "relu") %>%
  layer_batch_normalization() %>%
  layer_dropout(rate = FLAGS$dropout2) %>%
  layer_dense(units = FLAGS$nodes3, activation = "relu") %>%
  layer_batch_normalization() %>%
  layer_dropout(rate = FLAGS$dropout3) %>%
  layer_dense(units = 10, activation = "softmax")

summary(model)


history = model%>%compile(
  loss = 'categorical_crossentropy',
  metrics = c('accuracy'),
  optimizer = FLAGS$optimizer
) %>%
  fit(
    x = x_train,
    y = y_train,
    epochs = 35,
    batch_size = 128,
    validation_split = 0.2,
    callbacks = list(
      callback_early_stopping(patience = 5),
      callback_reduce_lr_on_plateau(factor = FLAGS$lr_annealing)
    ),
    verbose = TRUE
  )
  
  
  # Save the following code in .R file
  
  history = model%>%compile(
  loss = 'categorical_crossentropy',
  metrics = c('accuracy'),
  optimizer = FLAGS$optimizer
) %>%
  fit(
    x = x_train,
    y = y_train,
    epochs = 35,
    batch_size = 128,
    validation_split = 0.2,
    callbacks = list(
      callback_early_stopping(patience = 5),
      callback_reduce_lr_on_plateau(factor = FLAGS$lr_annealing)
    ),
    verbose = TRUE
  )
  
  # For Grid Search use the below function by giving the path of the above saved file.
  
  # using the tuning_run() function for grid search (I have doubt in this one that whatfile and path is used in attribute "file =")

runs <- tuning_run(file = "C:\\Users\\karan\\Desktop\\sample.R",
                   flags = list(
                     nodes1 = c(64, 128, 256),
                     nodes2 = c(64, 128, 256),
                     nodes3 = c(64, 128, 256),
                     dropout1 = c(0.2, 0.3, 0.4),
                     dropout2 = c(0.2, 0.3, 0.4),
                     dropout3 = c(0.2, 0.3, 0.4),
                     optimizer = c("rmsprop", "adam"),
                     lr_annealing = c(0.1, 0.05)
                   ),
                   sample = 0.003
)


runs %>% 
  filter(metric_val_loss == min(metric_val_loss)) %>% 
  glimpse()
runs$metric_loss


View(glimpse(runs))
  
  
